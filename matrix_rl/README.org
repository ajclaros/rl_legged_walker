#+title: Matrix Based RL

* Parametrization
Given a network of $M$ nodes and a maximum of $N$ parameters per node, we can create a $M\times N$ matrix to map all the parameters of a system. For example in the case of a 3 node ctrnn we can create:
|           | $N_0$ | $N_1$   | $N_2$ |
| w_{0j}    |       |         |       |
| w_{1j}    |       |         |       |
| w_{2j}    |       |         |       |
| b_{j}     |       |         |       |
| \tau_{j}  |       |         |       |

This enables the capacity for the learning rule to be applied generally to a heterogeneous system where all parameters (initial and max flux amplitudes, period ranges, parameter bounds) can be specifically tuned for each parameter. For this specific implementation, $\tau$'s are not learning parameters, therefore their associated positions associated for init_flux, max_flux are 0.

In the case of a heterogeneous system (systems that have different intrinsic properties such as neurons that learn at different rates, a system containing neurons and muscles,  or heterogeneous servo motors physical robotics) one can specify each parameter to have their own learning and convergence rates. Because this specific system is homogeneous learning and convergence rates are just floats.

- The script to run is single_trial.py
- All functions of learning rule are contained within learning_rule.py.

- RLCTRNN has two versions for updating. the functions denoted with a "2" contain manually implemented ring buffers whereas the original updating functions use np.roll.
- the manual implementation of the ring buffer decreases runtime by ~1.8 as np.roll creates a new array each time.

- The difference between CTRNN and RL_CTRNN is:
  + Step function uses the fluctuating weights
  + Step function calculates the next moment of the fluctuating weights
  + RL_CTRNN implements the reward and performance functions.







* Running run_trials.py
** Generating genomes
- If this is the first time the script is run, genomes will be generated. Parallel evolutionary runs will execute depending on the /num_processes/ variable.

- If the specified agent parameters do not exist within the /evolved/ folder, single_trial.py will create the folder and run an evolutionary algorithm to generate genomes
- If it does exist, the function checks if there is a minimum amount of genomes within a fitness range. If not, the evolutionary algorithm will run and save genomes within the specified range

** Running trials
- These genomes are starting points for learning algorithm to use
- Current state of the parameters will generally show learning for generator:RPG, size:3, neuron 0 driving leg movement
- Harder tasks are less stable at higher performance values (~0.5)

*** Plotting variables:
Variables of an agent can be tracked using /plot_vars/. Use the parameter "record_every" to dictate recording data every nth step. There are three tags associated with the agent:
- "track": Agent window variables (window_a, window_b, distance). Note that when recording performance the agent is recording performance as perceived by the agent, which is offset by a delay.
- "avg": Take the window's average and track it throughout the trial.
- "mat": tracks the agents matrix variables. Only tracks the variable associated in location [0,0]
*** Tolerance value
- Due to the nature of the agent's walking the specific walking cycle will have a different period length resulting in unequal averaging windows even though performance has not changed. This can be mitigated with larger window sizes (e.g. size 1000), but there will always be a different which leads to the agent learning off of inherent noise in the walking. To mitigate this, there is a "tolerance" parameter such that if the abs(reward) is lower than a specified tolerance there will be no reward administered, and the next moment will generate.
*** Premature convergence
- The fitness landscape for this task may lead to premature convergence in two ways.
  1. The agent explores in a wrong direction and rapidly drops in performance, if the drop is high enough, the maximum flux size will be achieved. When recovering with  a negative reward, the agent can likely recover to its original fitness, but the flux size then decreases close to 0
  2. As the agent increases its performance, the fluctuation decreases to zero before being close to the maximum fitness.
- To mitigate this, it may be beneficial to incorporate an additional growth function that  (a) Increases slowly initially and (b) Approaches a maximum value.
A slow growth is a proxy negative abs(reward)<value for a duration of time. A low absolute reward indicates that the agent's performance has not changed much in either a positive or negative direction. As the amount of time spent where the reward is low, the agent's flux size increases, but it will asymptotically approach a specified value. After the reward is greater than a specified value, this growth value then decreases to zero.
- It may be productive to use the "tolerance" as
- Tested function that satisfied the behavior of slow growth and level off value is the Gompertz curve which is sigmoidal
- A side effect of adding this function is less stable performance
$a e^{-b e^{-c x + d}}$ Where $a$ is the asymptotic value, $b$ is the displacement along the x axis, $c$ is the growth/level off rate.
Experiment with the function [[https://www.desmos.com/calculator/xgsorl8bdx][here]]
#+BEGIN_SRC python
tmp=0
a = 0.0001
b = 1.8
c = 0.001001
def run(tmp):
    #learning phase
    agent.stepRL(params["stepsize"])
    reward = agent.reward_func2(body.cx, learning=True)
    agent.sim.update_weights_with_reward(reward)
    if abs(reward) < params['tolerance']:
        # Gompertz curve performs best with CPG size 3, 3neuron config
        agent.sim.flux_mat += a * np.exp(-b * np.exp(-c * tmp + 0.5))
        # agent.sim.flux_mat = agent.sim.flux_mat * (np.log(tmp) * 0.001)
        tmp += 1
    elif tmp > 0:
        tmp -= 2
    return tmp
#+END_SRC
